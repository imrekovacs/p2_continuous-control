{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "#while True: \n",
    "# For untrained, a while True loop runs an eternity...\n",
    "for j in range(1):\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddpg_agent import Agent\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "agent = Agent(state_size=state_size, action_size=action_size, num_agents=num_agents, random_seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn(n_episodes=1000, max_t=1000, window_size=100, scores_threshold=30, print_every=10):\n",
    "    \"\"\"Runs Deep Q-Learning \n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        scores_threshold (int): set the threshold for average scores to stop training\n",
    "        print_every (int): print new line with scores every print_every episode.\n",
    "    \"\"\"\n",
    "    scores = []                        # scores from each episode will be stored here\n",
    "    scores_deque = deque(maxlen=window_size) \n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "        score = np.zeros(num_agents)\n",
    "        agent.reset()    \n",
    "        for t in range(max_t):\n",
    "            #Environment interaction block:\n",
    "            actions = agent.act(states)\n",
    "            env_info = env.step(actions)[brain_name]        \n",
    "            next_states = env_info.vector_observations   \n",
    "            rewards = env_info.rewards                   \n",
    "            dones = env_info.local_done        \n",
    "            agent.step(states, actions, rewards, next_states, dones)\n",
    "            states = next_states\n",
    "            score += rewards\n",
    "            if np.any(dones):\n",
    "                break \n",
    "                \n",
    "        scores_deque.append(np.mean(score))\n",
    "        scores.append(np.mean(score))\n",
    "        \n",
    "        average_score = np.mean(scores_deque)\n",
    "\n",
    "        print('\\rEpisode: {}\\tAverage Score: {:.2f}\\tEpi Mean Score: {:.2f}\\tEpi Min Score: {:.2f}\\tEpi Max Score: {:.2f}'.format(i_episode, average_score, np.mean(score), min(score), max(score)), end=\"\")\n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode: {}\\tAverage Score: {:.2f}\\tEpi Mean Score: {:.2f}\\tEpi Min Score: {:.2f}\\tEpi Max Score: {:.2f}'.format(i_episode, average_score, np.mean(score), min(score), max(score)))\n",
    "\n",
    "        if average_score >= scores_threshold:\n",
    "            print('\\nEnvironment solved in {} episodes!\\tAverage Score: {:.2f}'.format(i_episode, average_score))\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            break\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10\tAverage Score: 0.97\tEpi Mean Score: 1.64\tEpi Min Score: 0.52\tEpi Max Score: 3.51\n",
      "Episode: 20\tAverage Score: 2.02\tEpi Mean Score: 5.92\tEpi Min Score: 0.69\tEpi Max Score: 14.46\n",
      "Episode: 30\tAverage Score: 3.76\tEpi Mean Score: 8.75\tEpi Min Score: 1.96\tEpi Max Score: 23.61\n",
      "Episode: 40\tAverage Score: 5.83\tEpi Mean Score: 13.57\tEpi Min Score: 6.19\tEpi Max Score: 27.31\n",
      "Episode: 50\tAverage Score: 8.17\tEpi Mean Score: 22.58\tEpi Min Score: 18.63\tEpi Max Score: 30.51\n",
      "Episode: 60\tAverage Score: 10.94\tEpi Mean Score: 28.84\tEpi Min Score: 20.06\tEpi Max Score: 36.45\n",
      "Episode: 70\tAverage Score: 12.90\tEpi Mean Score: 25.34\tEpi Min Score: 12.20\tEpi Max Score: 34.12\n",
      "Episode: 80\tAverage Score: 15.08\tEpi Mean Score: 31.59\tEpi Min Score: 16.55\tEpi Max Score: 39.10\n",
      "Episode: 90\tAverage Score: 16.26\tEpi Mean Score: 28.85\tEpi Min Score: 22.22\tEpi Max Score: 35.55\n",
      "Episode: 100\tAverage Score: 17.87\tEpi Mean Score: 32.12\tEpi Min Score: 19.64\tEpi Max Score: 37.48\n",
      "Episode: 110\tAverage Score: 21.26\tEpi Mean Score: 33.64\tEpi Min Score: 9.15\tEpi Max Score: 38.708\n",
      "Episode: 120\tAverage Score: 24.33\tEpi Mean Score: 34.75\tEpi Min Score: 28.16\tEpi Max Score: 38.46\n",
      "Episode: 130\tAverage Score: 26.96\tEpi Mean Score: 34.31\tEpi Min Score: 26.19\tEpi Max Score: 39.18\n",
      "Episode: 140\tAverage Score: 28.84\tEpi Mean Score: 31.91\tEpi Min Score: 17.24\tEpi Max Score: 37.46\n",
      "Episode: 147\tAverage Score: 30.09\tEpi Mean Score: 34.02\tEpi Min Score: 26.04\tEpi Max Score: 39.63\n",
      "Environment solved in 147 episodes!\tAverage Score: 30.09\n"
     ]
    }
   ],
   "source": [
    "#Train my agent\n",
    "scores = dqn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Design Considerations and Improvement Ideas\n",
    "\n",
    "### Architecture\n",
    "The Deep DPG with an actor-critic architecture was chosen to solve this continuous control problem[10]. \n",
    "\n",
    "### Hyperparameter Selection\n",
    "#### Current Implementation\n",
    "\n",
    "##### Learning Rates\n",
    "The learning rates for critic and actor were set to 1e-4. With reduction of the network capacity (see sub-section below), the learning rates were increased to 1e-3 (Actor) and 1e-3 (Critic). The agents scores were oscillating, and it took 307 episodes to reach a score moving average of 30, as the agents periodically yielded very low mean episode scores, typically combined with a huge variance with a very low minimum episode score. For stability reason, the learning rates must be chosen well and may be non-equal. The critic's learning rate was then reduced to 3e-4, which indeed improved learning and stability, the score moving average was reached after 157 episodes, thus reducing  learning rate of the critic accelerated the agents learning. Reducing also actors learning LR_ACTOR to 3e-4 reduced the learning speed, reaching the goal after 263 episodes. With both LR_ACTOR and LR_CRITIC set to 3e-4, the agents got rewarded with higher minimum and maximum scores, also less oscillation was observed. The periodic collapsing of the scores were much less severe and the variation also seemed to decrease.\n",
    "\n",
    "\n",
    "##### Batch Size\n",
    "Deep Reinforcement Learning does allow higher batch sizes than supervised machine learning, like image classification. Higher batch sizes help to increase the learning of the agents, according to Stooke[8]. This was checked by increasing batch size from 128 to 256. The goal of a score moving average of 30 was reached after 263 episodes, but it took only 147 episodes having a batch size. Increasing the batch size indefinitely does not make sense either, hence there must be an optimimum. Stooke found the rule of thumb having a relation between learning rate and batch size: for the A2C increasing the learning rate with the square root of the batch size worked best. \n",
    "\n",
    "##### Weight Decay\n",
    "The weigth decay should lead to improved learning when having batch normalization. As no batch normalization is implemented, also weigth decay was set to zero[4]. \n",
    "\n",
    "#### Further  improvement\n",
    "The variance is still significant and the agents scores are still oscillating, see scores-plot. This could probably be further improved by adapting the learning rates and the batch size. \n",
    "\n",
    "\n",
    "### Network Capacity \n",
    "#### Current Implementation\n",
    "At the beginning, the actor and the critic were implemented having a bigger network capacity. The first implementations had a three stage neural network for both, actor and critic as well. The first fully-connected (fc1) had size of 512, the second fc2 had 256, and the third fc3 was 128. The training was very time consumining, 200-300 Episodes required 8-10 hours of training on a Intel Core i7 (9th generation) without GPU-support with the Hyper-V Virtual Machine running Ubuntu 18.04LTS and on a Intel Core i7 (9th generation) without GPU-support with Windows Subsystems for Linux (WSL) running  running Ubuntu 18.04LTS. The agents were very unstable, maybe a batch normalization could have improved here, but was not tried. Instead, the network capacities were successively reduced. Later on, the third stage was removed. This helped to stabilize, but also the improve learning. The current implementation is trained within less than hour.\n",
    "\n",
    "#### Further  improvement\n",
    "Probably, the neural network capacity could be reduced even further without compromising the average score and improving training speed. This is not investigated further.\n",
    "\n",
    "\n",
    "### Activation Functions\n",
    "#### Current Implementation\n",
    "As a first step, the model was implemented using standard ReLu-activation functions. The learning rate was very poor, using LeakyReLu as activation function instead of ReLu may improve learning.[1]\n",
    "In this project, using LeakyReLu helped to significantly improve the learning of the agents. The agents were simulated three times each\n",
    "- The Leaky-ReLu Agents did reach a moving average of min. 30 after 156 episodes\n",
    "- The ReLu-Agents required 207 episodes, almost 30% more episodes to get same average score\n",
    "\n",
    "The critic was also improved by adding a nonlinear activation to its output: \n",
    "A tanh-activation function was added to the output of the critic, further improving training speed.\n",
    "\n",
    "#### Further  improvement\n",
    "Wu et al [6] is suggesting to evaluate other output activation functions, i.e. replacing tanh()-function with a ReLu, LeakyReLu or with a Exponential Linear Unit (ELU), since sigmoids and tanh functions are limiting the output and hence the score.\n",
    "\n",
    "\n",
    "### Experience Replay Algorithm\n",
    "#### Current Implementation\n",
    "The implemented agent does have a experience replay algorithm:\n",
    "The Replay Buffer is used for experience replay to improve learning by learning, remembering and re-experiencing the past. \n",
    "In order to break temporal correlations, the experience replay is done by quasi-random and uniform sampling.  It also reduces the number of required experiences.[2]\n",
    "\n",
    "#### Further  improvement\n",
    "The experience replay algorithm was then improved by learning important experiences more often, thus prioritized and the experiences are not uniformly sampled and replayed anymore. It makes of the idea the same state-action pairs or tranisitions are more valuable in yielding higher rewards/scores and measured als Temporal-Difference error than others. \n",
    "So the neural network should be trained more with the better transitions[5], [9]. \n",
    "\n",
    "\n",
    "### Reproducibility and Seeding\n",
    "#### Current Implementation\n",
    "The scores and reproducibility of the trained agents depend heavily on the seed-value used as a training input. Reproducibility is a big issue in deep reinforcement learning and is subject intense of research. The dependency on the seeding value is a explainable as follows: the depending on the starting conditions (weight initialization and the random seed value) leads to different solutions. The neural network with the chosen cost function and optimizer could land in a local optimum and therefore could not improve scores anymore. Think of a human with the seed value being the earth coordinate, and his task is to find sweet/drinking water: One human is landing in a huge desert and may find a oasis with very limited water reserves. Another human would start somewhere in the northern hemisphere. He then would most probably find a creek providing virtual endless amount of fresh and clean drinking water.\n",
    "\n",
    "#### Further  improvement\n",
    "Reproducibility and dependency on seeding can be improved by improving the weight initialization and the by applying batch normalization. [3]\n",
    "\n",
    "\n",
    "### Weight Initialization\n",
    "#### Current Implementation\n",
    "The implemented networks are initialized with uniformly distributed values by using the function self.weight.data.uniform_(-stdv, stdv), with stdv=3e-3.\n",
    "\n",
    "\n",
    "#### Further  improvement\n",
    "The neural net weight initialization is crucial for convergence and training speed. There are a lot of research studies on this topic. A more recent way to initialize the weights is named after his inventor Kaiming. Sometimes it also called Xavier-initialization. \n",
    "\n",
    "T.Salimans[7] applied weight normalization to their DQN-agent and demonstrated some improvements in scores and learning.\n",
    "\n",
    "\n",
    "### Batch Norm\n",
    "#### Current Implementation\n",
    "The current implementation does not include batch normalization. The distribution of each input layer is changing with each training sample and between each batch. This makes training and convergence hard. \n",
    " \n",
    "#### Further  improvement\n",
    "Ioffe and Szegedy[4] refer the ever changing distribution to as the covariate shift problem. They suggest to apply batch normalization in order to reduce the input variance of the distribution at each input. They claim that applying batch normalization helps to make the weight initialisation less criticial. In this case, it would eventually help to improve reproducibility and reduce dependability on the seed value. The batch normalization should be combined with the weight decaying.\n",
    " \n",
    " \n",
    "### References\n",
    "- [1] \"Deep Reinforcement Learning that Matters\", 2019, P.Henderson, et al.\n",
    "- [2] \"Self-improving reactive agents based on reinforcement learning, planning and teaching\", 1992, L.-J. Lin\n",
    "- [3] \"Let's Play Again: Variability of Deep Reinforcement Learning Agents in Atari Environments\", 2018, K.Clary, et al.\n",
    "- [4] \"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\", S.Ioffe and C.Szegedy\n",
    "- [5] \"Prioritized experience replay\", 2015, T. Schaul, J. Quan, I. Antonoglou, and D. Silver\n",
    "- [6] \"ANS: Adaptive Network Sacling for Deep Rectifier Reinforcement Learning Models\", 2018, Y.-H. Wu, et al.\n",
    "- [7] \"Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks\", 2016, T.Salmans and D. P. Kingma \n",
    "- [8] \"Accelerated methods for Deep Reinforcement Learning\", 2019, A. Stooke and P. Abbeel\n",
    "- [9] \"Distributed Prioritized Experience Replay\", 2018, D.Horgan et al.\n",
    "- [10]\"Continuous Control with deep reinforcement learning\", 2019, T.P.Lilicrap et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
